{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=top></div>\n",
    "\n",
    "# Reinforcement Learning with Doom - Increasing complexity and monitoring the model\n",
    "\n",
    "Leandro Kieliger\n",
    "contact@lkieliger.ch\n",
    "\n",
    "---\n",
    "## Description\n",
    "\n",
    "In this notebook we are going to build upon the setup introduced in the previous part of this series. We will tackle a more difficult scenario, add useful logging to the learning process and finally start competing against in-game bots in a first attemps at playing a deathmatch game using reinforcement learning.\n",
    "\n",
    "The notebook is structured in 3 parts:\n",
    "\n",
    "\n",
    "### [Part 1 - Increasing complexity](#part_1)\n",
    "* [Defend the center](#defend_the_center)\n",
    "* [Defend the center (harder)](#defend_the_center_hard)\n",
    "\n",
    "    \n",
    "### [Part 2 - Monitoring the model](#part_2)\n",
    "* [Adding hooks](#monitoring)\n",
    "* [Monitoring callback](#callback)\n",
    "* [Adding normalization](#normalization)\n",
    "* [Comparing norms](#norm_comparison)\n",
    "    \n",
    "    \n",
    "### [Part 3 - Playing doom deathmatch](#part_3)\n",
    "* [Scenario presentation](#deathmatch_presentation)\n",
    "* [New environment](#deathmatch_environment)\n",
    "* [Discussion](#deathmatch_discussion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=part_1></div>\n",
    "\n",
    "# [^](#top) Part 1 - Increasing complexity\n",
    "\n",
    "\n",
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import cv2\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import typing as t\n",
    "import vizdoom\n",
    "from stable_baselines3 import ppo\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common import evaluation, policies\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "from common import envs, plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env, spaces\n",
    "from stable_baselines3.common import vec_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.ppo import ppo\n",
    "from vizdoom import GameVariable\n",
    "\n",
    "from common.models import init_model\n",
    "from common.monitoring import LayerActivationMonitoring\n",
    "from common.utils import get_available_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# image_path1 = \"/Users/ericsunkuan/Desktop/NTUEE/112-1/RL/final_project/vizdoom/ViZDoom-master/some_tests/game_image_save/no_enemy_sample.jpeg\"\n",
    "# base64_image1 = encode_image(image_path1)\n",
    "\n",
    "# client = OpenAI(api_key = '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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results in a 100x156 image, no pixel lost due to padding with the default CNN architecture\n",
    "frame_processor = lambda frame: cv2.resize(frame[40:, 4:-4], None, fx=.5, fy=.5, interpolation=cv2.INTER_AREA)\n",
    "#frame_processor = lambda frame: cv2.resize(frame, None, fx=.5, fy=.5, interpolation=cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import typing as t\n",
    "\n",
    "from vizdoom import Button\n",
    "\n",
    "\n",
    "# # Buttons that cannot be used together\n",
    "# MUTUALLY_EXCLUSIVE_GROUPS = [\n",
    "#     [Button.MOVE_RIGHT, Button.MOVE_LEFT],\n",
    "#     [Button.TURN_RIGHT, Button.TURN_LEFT],\n",
    "#     [Button.MOVE_FORWARD, Button.MOVE_BACKWARD],\n",
    "# ]\n",
    "\n",
    "# # Buttons that can only be used alone.\n",
    "# EXCLUSIVE_BUTTONS = [Button.ATTACK]\n",
    "\n",
    "\n",
    "# def has_exclusive_button(actions: np.ndarray, buttons: np.array) -> np.array:\n",
    "#     exclusion_mask = np.isin(buttons, EXCLUSIVE_BUTTONS)\n",
    "    \n",
    "#     # Flag actions that have more than 1 active button among exclusive list.\n",
    "#     return (np.any(actions.astype(bool) & exclusion_mask, axis=-1)) & (np.sum(actions, axis=-1) > 1)\n",
    "\n",
    "\n",
    "# def has_excluded_pair(actions: np.ndarray, buttons: np.array) -> np.array:\n",
    "#     # Create mask of shape (n_mutual_exclusion_groups, n_available_buttons), marking location of excluded pairs.\n",
    "#     mutual_exclusion_mask = np.array([np.isin(buttons, excluded_group) \n",
    "#                                       for excluded_group in MUTUALLY_EXCLUSIVE_GROUPS])\n",
    "\n",
    "#     # Flag actions that have more than 1 button active in any of the mutual exclusion groups.\n",
    "#     return np.any(np.sum(\n",
    "#         # Resulting shape (n_actions, n_mutual_exclusion_groups, n_available_buttons)\n",
    "#         (actions[:, np.newaxis, :] * mutual_exclusion_mask.astype(int)),\n",
    "#         axis=-1) > 1, axis=-1)\n",
    "\n",
    "\n",
    "# def get_available_actions(buttons: np.array) -> t.List[t.List[float]]:\n",
    "#     # Create list of all possible actions of size (2^n_available_buttons x n_available_buttons)\n",
    "#     action_combinations = np.array([list(seq) for seq in itertools.product([0., 1.], repeat=len(buttons))])\n",
    "\n",
    "#     # Build action mask from action combinations and exclusion mask\n",
    "#     illegal_mask = (has_excluded_pair(action_combinations, buttons)\n",
    "#                     | has_exclusive_button(action_combinations, buttons))\n",
    "\n",
    "#     possible_actions = action_combinations[~illegal_mask]\n",
    "#     possible_actions = possible_actions[np.sum(possible_actions, axis=1) > 0]  # Remove no-op\n",
    "\n",
    "#     print('Built action space of size {} from buttons {}'.format(len(possible_actions), buttons))\n",
    "#     return possible_actions.tolist()\n",
    "\n",
    "# possible_actions = get_available_actions(np.array([\n",
    "#     Button.ATTACK, Button.MOVE_FORWARD, Button.MOVE_LEFT, \n",
    "#     Button.MOVE_RIGHT, Button.TURN_LEFT, Button.TURN_RIGHT]))\n",
    "\n",
    "# possible_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model in this more challenging environment will definitely require more training time. This is a good opportunity to log some metrics to help us monitor the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from functools import partial\n",
    "\n",
    "class Hook:\n",
    "    \"\"\"Wrapper for PyTorch forward hook mechanism.\"\"\"\n",
    "    def __init__(self, module: nn.Module, func: t.Callable):\n",
    "        self.hook = None            # PyTorch's hook.\n",
    "        self.module = module        # PyTorch layer to which the hook is attached to.\n",
    "        self.func = func            # Function to call on each forward pass.\n",
    "        self.register()\n",
    "\n",
    "    def register(self):\n",
    "        self.activation_data = deque(maxlen=1024)\n",
    "        self.hook = self.module.register_forward_hook(partial(self.func, self))\n",
    "\n",
    "    def remove(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "def store_activation(hook, module, inp, outp):\n",
    "    \"\"\"Function intented to be called by a hook on a forward pass.\n",
    "    \n",
    "    Args:\n",
    "        hook:    The hook object that generated the call.\n",
    "        module:  The module on which the hook is registered.\n",
    "        inp:     Input of the module.\n",
    "        outp:    Output of the module.\n",
    "    \"\"\"\n",
    "    hook.activation_data.append(outp.data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "\n",
    "def get_low_act(data, threshold=0.2):\n",
    "    \"\"\"Computes the proportion of activations that have value close to zero.\"\"\"\n",
    "    low_activation = ((-threshold <= data) & (data <= threshold))\n",
    "    return np.count_nonzero(low_activation) / np.size(low_activation)\n",
    "\n",
    "\n",
    "# Callback for periodic logging to tensorboard.\n",
    "class LayerActivationMonitoring(BaseCallback):\n",
    "    \n",
    "    def _on_rollout_start(self) -> None:\n",
    "        \"\"\"Called after the training phase.\"\"\"\n",
    "        \n",
    "        hooks = self.model.policy.features_extractor.hooks\n",
    "        \n",
    "        # Remove the hooks so that they don't get called for rollout collection.\n",
    "        for h in hooks: h.remove() \n",
    "\n",
    "        # Log last datapoint and statistics to tensorboard.\n",
    "        for i, hook in enumerate(hooks):\n",
    "            if len(hook.activation_data) > 0:\n",
    "                data = hook.activation_data[-1]\n",
    "                self.logger.record(f'diagnostics/activation_l{i}', data)\n",
    "                self.logger.record(f'diagnostics/mean_l{i}', np.mean(data))\n",
    "                self.logger.record(f'diagnostics/std_l{i}', np.std(data))\n",
    "                self.logger.record(f'diagnostics/low_act_prop_l{i}', get_low_act(data))\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \"\"\"Called before the training phase.\"\"\"\n",
    "        for h in self.model.policy.features_extractor.hooks: h.register()\n",
    "\n",
    "    def _on_step(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define a function that will attach hooks to every activation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_hooks(model):\n",
    "    model.policy.features_extractor.hooks = [\n",
    "        Hook(layer, store_activation)\n",
    "        for layer in model.policy.features_extractor.cnn\n",
    "        if isinstance(layer, nn.ReLU) or isinstance(layer, nn.LeakyReLU)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_net(m: nn.Module):\n",
    "    if len(m._modules) > 0:\n",
    "        for subm in m._modules:\n",
    "            init_net(m._modules[subm])\n",
    "    elif isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(\n",
    "            m.weight, \n",
    "            a=0.1,         # Same as the leakiness parameter for LeakyReLu.\n",
    "            mode='fan_in', # Preserves magnitude in the forward pass.\n",
    "            nonlinearity='leaky_relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=normalization></div>\n",
    "\n",
    "## Normalization\n",
    "\n",
    "To add layer normalization, we need to redefine a model. We will then tell stable-baselines to use our customized model instead of the default one. For the sake of brevity, sizes for the different layers of our model have been hardcoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "class CustomCNN(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 128, **kwargs):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.LayerNorm([3, 100, 156]),\n",
    "            \n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4, padding=0, bias=False),\n",
    "            nn.LayerNorm([32, 24, 38]),\n",
    "            nn.LeakyReLU(**kwargs),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0, bias=False),\n",
    "            nn.LayerNorm([64, 11, 18]),\n",
    "            nn.LeakyReLU(**kwargs),\n",
    "            \n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0, bias=False),\n",
    "            nn.LayerNorm([64, 9, 16]),\n",
    "            nn.LeakyReLU(**kwargs),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(9216, features_dim, bias=False),\n",
    "            nn.LayerNorm(features_dim),\n",
    "            nn.LeakyReLU(**kwargs),\n",
    "        )\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        return self.linear(self.cnn(observations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's collect some data again but this time using the modified model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(model):\n",
    "    register_hooks(model)\n",
    "    init_net(model.policy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "        self.fq_count=check_freq\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls >= self.fq_count:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "            print(f'best_model_{self.n_calls} save!')\n",
    "            self.fq_count+=self.check_freq\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Map presentation](figures/flatmap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=deathmatch_environment></div>\n",
    "\n",
    "## Modified environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewards\n",
    "# 1 per kill\n",
    "reward_factor_frag = 1.0\n",
    "reward_factor_damage = 0.01\n",
    "\n",
    "# Player can move at ~16.66 units per tick\n",
    "reward_factor_distance = 5e-4\n",
    "penalty_factor_distance = -2.5e-3\n",
    "reward_threshold_distance = 3.0\n",
    "\n",
    "# Pistol clips have 10 bullets\n",
    "reward_factor_ammo_increment = 0.02\n",
    "reward_factor_ammo_decrement = -0.01\n",
    "\n",
    "# Player starts at 100 health\n",
    "reward_factor_health_increment = 0.02\n",
    "reward_factor_health_decrement = -0.01\n",
    "reward_factor_armor_increment = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import spaces\n",
    "from vizdoom.vizdoom import GameVariable\n",
    "\n",
    "class DoomWithBots(envs.DoomEnv):\n",
    "\n",
    "    \"\"\"\n",
    "    define the logic of when to prompt LLM\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, game, frame_processor, frame_skip, n_bots):\n",
    "        super().__init__(game, frame_processor, frame_skip)\n",
    "        self.n_bots = n_bots\n",
    "        self.last_frags = 0    \n",
    "        #self._reset_bots()\n",
    "        \n",
    "        # Redefine the action space using combinations.\n",
    "        # self.possible_actions = get_available_actions(np.array(game.get_available_buttons()))\n",
    "        # self.action_space = spaces.Discrete(len(self.possible_actions))\n",
    "\n",
    "        print(game.get_available_buttons_size())\n",
    "        self.action_space = spaces.Discrete(game.get_available_buttons_size())\n",
    "\n",
    "        # self.sampled_actions_done = False\n",
    "        # self.cur_actions = 0  ### the current action network num\n",
    "        self.item_count = 0\n",
    "        self.health_count = 0\n",
    "        self.ammo = 0\n",
    "        self.isdead = False\n",
    "        self.recordreward = 0\n",
    "\n",
    "        self.last_health = 100\n",
    "        self.last_x, self.last_y = self._get_player_pos()\n",
    "        #self.ammo_state = self._get_ammo_state()\n",
    "        self.last_ammo = 50\n",
    "        self.last_armor = 0\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        ### prompt LLM if actions are done :\n",
    "        #if self.sampled_actions_done:\n",
    "\n",
    "        \n",
    "        #print(action)\n",
    "        actions = np.zeros(self.game.get_available_buttons_size())\n",
    "        actions[action] = 1\n",
    "        reward = self.game.make_action(actions, self.frame_skip)\n",
    "        \n",
    "        # self.game.make_action(self.possible_actions[action], self.frame_skip)\n",
    "       \n",
    "        # Compute rewards.\n",
    "        # frags = self.game.get_game_variable(GameVariable.FRAGCOUNT)\n",
    "        # reward = frags - self.last_frags\n",
    "        # self.last_frags = frags\n",
    "        #reward = 0\n",
    "        self.isdead = self._respawn_if_dead()\n",
    "        #reward += self.get_reward(action)\n",
    "        reward += self.shape_rewards()\n",
    "        if action == 5:\n",
    "            reward -= 4\n",
    "\n",
    "        info = {\"item_count\":0,\"death_count\":0,\"ammo\":0}\n",
    "\n",
    "        # Check for episode end.\n",
    "\n",
    "        \n",
    "        if self.isdead:\n",
    "        #     reward -= 20\n",
    "            self.isdead = False\n",
    "        if self.justreseted:\n",
    "            #reward -= -20\n",
    "            self.justreseted = False\n",
    "        done = self.game.is_episode_finished()\n",
    "        self.state = self._get_frame(done)\n",
    "        self.recordreward += reward\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    def get_reward(self,action=None):\n",
    "        reward = 0\n",
    "        item_count=self.item_count\n",
    "        health_count=self.health_count\n",
    "        ammo_count = self.ammo\n",
    "        if self.game.get_state():\n",
    "            #print(self.game.get_state().game_variables)\n",
    "            item_count = self.game.get_state().game_variables[8]\n",
    "            health_count = self.game.get_state().game_variables[1]\n",
    "            ammo_count = self.game.get_state().game_variables[0]\n",
    "            #print(item_count,',',health_count,',',ammo_count)\n",
    "            '''\n",
    "            labels_buffer = self.game.get_state().labels_buffer\n",
    "            wall = np.count_nonzero(labels_buffer==0)\n",
    "            if wall/(640*480) >=0.9:\n",
    "                if action == 3 :\n",
    "                    reward +=0.3\n",
    "            '''\n",
    "        # if(self.isdead == False and health_count > self.health_count):\n",
    "        #     reward += 200 * (health_count - self.health_count)\n",
    "        # if(item_count > self.item_count):\n",
    "        #     reward += (item_count-self.item_count)\n",
    "        #reward += 4 * (ammo_count - self.ammo) \n",
    "        self.item_count = item_count\n",
    "        self.health_count = health_count\n",
    "        self.ammo = ammo_count\n",
    "        if action == 5:\n",
    "            reward -= 4\n",
    "        return reward\n",
    "    \n",
    "\n",
    "    def shape_rewards(self):\n",
    "        reward_contributions = [\n",
    "            self._compute_ammo_reward(),\n",
    "            self._compute_health_reward(),\n",
    "            self._compute_armor_reward(),\n",
    "            #self._compute_distance_reward(*self._get_player_pos()),\n",
    "        ]\n",
    "\n",
    "        return sum(reward_contributions)\n",
    "\n",
    "    def reset(self):\n",
    "        #self._reset_bots()\n",
    "        #self.last_frags = 0\n",
    "        # ammo = self.game.get_state().game_variables[0]\n",
    "        # info = {\"info\":ammo}\n",
    "        #self.last_x, self.last_y = self._get_player_pos()\n",
    "        self.last_health = 100\n",
    "        self.last_armor = 0\n",
    "        self.last_ammo = 50\n",
    "        print(self.recordreward)\n",
    "        self.recordreward = 0\n",
    "        self.justreseted = True\n",
    "        return super().reset()\n",
    "    \n",
    "    def _compute_distance_reward(self, x, y):\n",
    "        \"\"\"Computes a reward/penalty based on the distance travelled since last update.\"\"\"\n",
    "        dx = self.last_x - x\n",
    "        dy = self.last_y - y\n",
    "\n",
    "        distance = np.sqrt(dx ** 2 + dy ** 2)\n",
    "\n",
    "        if distance - reward_threshold_distance > 0:\n",
    "            reward = reward_factor_distance\n",
    "        else:\n",
    "            reward = -reward_factor_distance\n",
    "\n",
    "        self.last_x = x\n",
    "        self.last_y = y\n",
    "        #self._log_reward_stat('distance', reward)\n",
    "\n",
    "        return reward\n",
    "    \n",
    "    def _compute_health_reward(self):\n",
    "        \"\"\"Computes a reward/penalty based on total health change since last update.\"\"\"\n",
    "        # When the player is dead, the health game variable can be -999900\n",
    "        health = max(self.game.get_game_variable(GameVariable.HEALTH), 0)\n",
    "\n",
    "        health_reward = reward_factor_health_increment * max(0, health - self.last_health)\n",
    "        health_penalty = reward_factor_health_decrement * min(0, health - self.last_health)\n",
    "        reward = health_reward - health_penalty\n",
    "\n",
    "        self.last_health = health\n",
    "        #self._log_reward_stat('health', reward)\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def _compute_armor_reward(self):\n",
    "        \"\"\"Computes a reward/penalty based on total armor change since last update.\"\"\"\n",
    "        armor = self.game.get_game_variable(GameVariable.ARMOR)\n",
    "        reward = reward_factor_armor_increment * max(0, armor - self.last_armor)\n",
    "        \n",
    "        self.last_armor = armor\n",
    "        #self._log_reward_stat('armor', reward)\n",
    "\n",
    "        return reward\n",
    "    \n",
    "    def _compute_ammo_reward(self):\n",
    "        ammo = self.last_ammo\n",
    "        if self.game.get_state():\n",
    "            ammo = self._get_ammo_state()\n",
    "        if self.justreseted or self.isdead:\n",
    "            reward = 0\n",
    "            self.last_ammo = 50\n",
    "        else:\n",
    "            reward = 0.2 * (ammo - self.last_ammo)\n",
    "            self.last_ammo = ammo\n",
    "        \n",
    "        return reward\n",
    "        \n",
    "\n",
    "    \n",
    "    def _get_player_pos(self):\n",
    "        \"\"\"Returns the player X- and Y- coordinates.\"\"\"\n",
    "        return self.game.get_game_variable(GameVariable.POSITION_X), self.game.get_game_variable(\n",
    "            GameVariable.POSITION_Y)\n",
    "\n",
    "    def _get_ammo_state(self):\n",
    "        \"\"\"Returns the total available ammunition per weapon slot.\"\"\"\n",
    "        ammo = 0\n",
    "        ammo = self.game.get_state().game_variables[0]\n",
    "        return ammo\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _respawn_if_dead(self):\n",
    "        if not self.game.is_episode_finished():\n",
    "            if self.game.is_player_dead():\n",
    "                self.game.respawn_player()\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "                \n",
    "    def _reset_bots(self):\n",
    "        # Make sure you have the bots.cfg file next to the program entry point.\n",
    "        self.game.send_game_command('removebots')\n",
    "        for i in range(self.n_bots):\n",
    "            self.game.send_game_command('addbot')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other adaptation required is to pass several game arguments via the `add_game_args` function when creating the actual VizDoom game instance. We also redefine the function creating vectorized environments so that it uses the newly defined wrapper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import VecTransposeImage, DummyVecEnv\n",
    "\n",
    "def env_with_bots(scenario, render = False, **kwargs) -> envs.DoomEnv:\n",
    "    # Create a VizDoom instance.\n",
    "    game = vizdoom.DoomGame()\n",
    "    game.load_config('scenarios/bots_deathmatch_multimaps.cfg')\n",
    "    game.load_config(f'scenarios/{scenario}.cfg')\n",
    "    # game.add_game_args('-host 1 -deathmatch +viz_nocheat 0 +cl_run 1 +name AGENT +colorset 0' +\n",
    "    #                     '+sv_forcerespawn 1 +sv_respawnprotect 1 +sv_nocrouch 1 +sv_noexit 1')      # Players can't crouch.\n",
    "    \n",
    "    game.set_window_visible(render)\n",
    "    game.set_labels_buffer_enabled(True)\n",
    "    game.set_death_penalty(2)\n",
    "    #game.setWindowVisible(render)\n",
    "    game.init()\n",
    "\n",
    "    return DoomWithBots(game, **kwargs)\n",
    "\n",
    "def vec_env_with_bots(n_envs=1,render = False, **kwargs) -> VecTransposeImage:\n",
    "    return VecTransposeImage(DummyVecEnv([lambda: env_with_bots(render = render,**kwargs)] * n_envs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it, we have all the elements required to play Doom deathmatch! Let us define one final time the environment and agent arguments and launch the training process using our latest model.\n",
    "\n",
    "Training an agent to play deathmatch is significantly harder than the previous scenarios we have tried so far. We will need to be patient and train for a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 255, (100, 156, 3), uint8)\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericsunkuan/anaconda3/envs/merge_test2/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 255, (100, 156, 3), uint8)\n",
      "6\n",
      "Box(0, 255, (100, 156, 3), uint8)\n",
      "6\n",
      "Box(0, 255, (100, 156, 3), uint8)\n",
      "6\n",
      "Box(0, 255, (100, 156, 3), uint8)\n",
      "6\n",
      "Using cpu device\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Logging to logs/tensorboard/bots_deathmatch_multimaps_35\n",
      "-853.0000000000001\n",
      "-920.9999999999999\n",
      "-961.0000000000001\n",
      "-1205.0\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 157   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 103   |\n",
      "|    total_timesteps | 16384 |\n",
      "------------------------------\n",
      "-648.9800000000001\n",
      "-2714.38\n",
      "-2414.26\n",
      "-175.2\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 99         |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 327        |\n",
      "|    total_timesteps      | 32768      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06837724 |\n",
      "|    clip_fraction        | 0.534      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.65      |\n",
      "|    explained_variance   | -0.00223   |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 7.26       |\n",
      "|    n_updates            | 3          |\n",
      "|    policy_gradient_loss | -0.00379   |\n",
      "|    value_loss           | 14.9       |\n",
      "----------------------------------------\n",
      "-1613.9\n",
      "best_model_10000 save!\n",
      "-981.8499999999995\n",
      "-1026.0299999999995\n",
      "-1153.85\n",
      "-137.80000000000007\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 565         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044336833 |\n",
      "|    clip_fraction        | 0.39        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.53       |\n",
      "|    explained_variance   | 0.194       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 3.86        |\n",
      "|    n_updates            | 6           |\n",
      "|    policy_gradient_loss | -0.00671    |\n",
      "|    value_loss           | 7.56        |\n",
      "-----------------------------------------\n",
      "-518.7699999999995\n",
      "-119.20000000000007\n",
      "-73.2\n",
      "-385.3999999999996\n",
      "-538.3799999999997\n",
      "-94.80000000000005\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 81         |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 804        |\n",
      "|    total_timesteps      | 65536      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04122372 |\n",
      "|    clip_fraction        | 0.366      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.48      |\n",
      "|    explained_variance   | 0.543      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 1.15       |\n",
      "|    n_updates            | 9          |\n",
      "|    policy_gradient_loss | -0.0043    |\n",
      "|    value_loss           | 4.31       |\n",
      "----------------------------------------\n",
      "-52.00000000000001\n",
      "-52.40000000000002\n",
      "-53.20000000000003\n",
      "-55.560000000000024\n",
      "-23.799999999999994\n",
      "-57.400000000000006\n",
      "-56.56000000000002\n",
      "best_model_20000 save!\n",
      "-31.999999999999993\n",
      "-40.6\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 78          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 1046        |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037943043 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.673       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.16        |\n",
      "|    n_updates            | 12          |\n",
      "|    policy_gradient_loss | 0.00373     |\n",
      "|    value_loss           | 1.95        |\n",
      "-----------------------------------------\n",
      "-11.399999999999999\n",
      "-7.200000000000001\n",
      "-15.399999999999995\n",
      "-36.4\n",
      "-19.6\n",
      "-23.999999999999993\n",
      "-11.399999999999997\n",
      "-14.599999999999998\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 76         |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 1285       |\n",
      "|    total_timesteps      | 98304      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02751168 |\n",
      "|    clip_fraction        | 0.313      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.44      |\n",
      "|    explained_variance   | 0.635      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.33       |\n",
      "|    n_updates            | 15         |\n",
      "|    policy_gradient_loss | -0.00696   |\n",
      "|    value_loss           | 0.727      |\n",
      "----------------------------------------\n",
      "-14.399999999999997\n",
      "-11.4\n",
      "-33.4\n",
      "-24.0\n",
      "-32.199999999999996\n",
      "-11.399999999999997\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 75          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 1524        |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023079377 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.774       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0201      |\n",
      "|    n_updates            | 18          |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    value_loss           | 0.29        |\n",
      "-----------------------------------------\n",
      "-11.399999999999999\n",
      "-11.399999999999999\n",
      "-11.399999999999997\n",
      "best_model_30000 save!\n",
      "-7.2\n",
      "-6.199999999999999\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 74          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 1758        |\n",
      "|    total_timesteps      | 131072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015219684 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.689       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.101       |\n",
      "|    n_updates            | 21          |\n",
      "|    policy_gradient_loss | -0.00721    |\n",
      "|    value_loss           | 0.235       |\n",
      "-----------------------------------------\n",
      "-11.399999999999995\n",
      "-15.599999999999998\n",
      "-7.2\n",
      "-7.2\n",
      "-11.399999999999999\n",
      "-15.599999999999998\n",
      "-15.6\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 74          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 1991        |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015788969 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.642       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.173       |\n",
      "|    n_updates            | 24          |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    value_loss           | 0.093       |\n",
      "-----------------------------------------\n",
      "-7.200000000000001\n",
      "0.0\n",
      "-11.399999999999999\n",
      "best_model_40000 save!\n"
     ]
    }
   ],
   "source": [
    "# Define new environment parameters.\n",
    "env_args = {\n",
    "    'scenario': 'bots_deathmatch_multimaps',\n",
    "    'frame_skip': 1,\n",
    "    'frame_processor': frame_processor,\n",
    "    'n_bots': 8\n",
    "}\n",
    "\n",
    "# Defines new agent parameters.\n",
    "agent_args = {\n",
    "    'n_epochs': 3,\n",
    "    'n_steps': 4096,\n",
    "    'learning_rate': 1e-4,\n",
    "    'batch_size': 32,\n",
    "    'policy_kwargs': {'features_extractor_class': CustomCNN}\n",
    "}\n",
    "\n",
    "# Create environments with bots.\n",
    "env = vec_env_with_bots(4,render=True, **env_args)\n",
    "eval_env = vec_env_with_bots(1,render=False, **env_args)\n",
    "\n",
    "# Build the agent.\n",
    "#agent = ppo.PPO(CustomCNN, env, tensorboard_log='logs/tensorboard', seed=42, **agent_args)\n",
    "agent = ppo.PPO(policies.ActorCriticCnnPolicy, env, tensorboard_log='logs/tensorboard', seed=0, verbose = 1, **agent_args)\n",
    "init_model(agent)\n",
    "#agent = agent.load('/Users/ericsunkuan/Desktop/NTUEE/112-1/RL/final_project/fp_merged/rl-doom/standalone_examples2/finalmap_navi/models/navi_1210_7_models/best_model_270000',env)\n",
    "\n",
    "\n",
    "# Create callbacks.\n",
    "monitoring_callback = LayerActivationMonitoring()\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env, \n",
    "    n_eval_episodes=10, \n",
    "    eval_freq=16384, \n",
    "    log_path=f'logs/evaluations/{env_args[\"scenario\"]}',\n",
    "    best_model_save_path=f'logs/models/{env_args[\"scenario\"]}')\n",
    "\n",
    "savepath = \"/Users/ericsunkuan/Desktop/NTUEE/112-1/RL/final_project/fp_merged/rl-doom/standalone_examples2/finalmap_navi/models\"\n",
    "savecallback = TrainAndLoggingCallback(10000, save_path = savepath, verbose= 1)\n",
    "\n",
    "# Start the training process.\n",
    "agent.learn(total_timesteps=3000000, tb_log_name=env_args['scenario'], callback=savecallback)\n",
    "\n",
    "# Cleanup.\n",
    "env.close()\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 255, (100, 156, 3), uint8)\n",
      "6\n",
      "0\n",
      "-283.68300000000136\n",
      "0\n",
      "-372.84350000000205\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "\n",
    "env_args_gif = {\n",
    "    'scenario': 'bots_deathmatch_multimaps',\n",
    "    'frame_skip': 4,\n",
    "    'frame_processor': frame_processor,\n",
    "    'n_bots': 8\n",
    "}\n",
    "\n",
    "def make_gif(agent, file_path):\n",
    "    env = vec_env_with_bots(1,render=True, **env_args_gif)\n",
    "    env.venv.envs[0].game.set_seed(0)\n",
    "\n",
    "    \n",
    "    images = []\n",
    "\n",
    "    for i in range(2):\n",
    "        obs = env.reset()\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = agent.predict(obs)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            images.append(env.venv.envs[0].game.get_state().screen_buffer)\n",
    "\n",
    "    imageio.mimsave(file_path, images, fps=35)\n",
    "\n",
    "    env.close()\n",
    "\n",
    "make_gif(agent, '/Users/ericsunkuan/Desktop/NTUEE/112-1/RL/final_project/fp_merged/rl-doom/standalone_examples2/gifs/navi1.gif')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how did the agent perform? Plotting the average reward over time depicts a tragic situation: although there seems to be some improvement over time, the performance of our model is really not great. Even after 2 million steps the agent barely reaches 2 frags per match on average. Compare that to the best bot which has around 13 frags at the end of each game and our objective seem still pretty far away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting.plot_evaluation_results('logs/evaluations/deathmatch_simple/evaluations.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=deathmatch_discussion></div>\n",
    "\n",
    "# Discussion\n",
    "\n",
    "So the learning process did not go as smoothly as one could have expected. Why is that?\n",
    "\n",
    "The issue lies in the fact that the rewards are very sparse. In other words, only a few combinations of state and action generate useful signals that our agent can use for learning. Indeed, to obtain a frag, the agent has to perform several steps \"just right\". It has to move and track ennemies, repeatedly shoot them until eventually their health reaches zero.\n",
    "\n",
    "The way to solve this issue is by means of \"reward shaping\". The idea is simple: give small positive reward to action that we believe will help towards our main objective of obtaining frags. For example, we can give rewards proportional to damage inflicted to ennemies or proportional to the ammo and health collected.\n",
    "\n",
    "Another option to help with the learning process is to design some learning curriculum. The idea here is to simplify the task early in the learning process and gradually increase the difficulty. For example, we could reduce the speed and the health of ennemies to make it easier for our agent to obtain positive rewards.\n",
    "\n",
    "We will implement these ideas in the next part of this series to get much better results so stay tuned!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
