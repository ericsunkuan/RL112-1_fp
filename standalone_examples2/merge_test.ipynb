{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=top></div>\n",
    "\n",
    "# Reinforcement Learning with Doom - Increasing complexity and monitoring the model\n",
    "\n",
    "Leandro Kieliger\n",
    "contact@lkieliger.ch\n",
    "\n",
    "---\n",
    "## Description\n",
    "\n",
    "In this notebook we are going to build upon the setup introduced in the previous part of this series. We will tackle a more difficult scenario, add useful logging to the learning process and finally start competing against in-game bots in a first attemps at playing a deathmatch game using reinforcement learning.\n",
    "\n",
    "The notebook is structured in 3 parts:\n",
    "\n",
    "\n",
    "### [Part 1 - Increasing complexity](#part_1)\n",
    "* [Defend the center](#defend_the_center)\n",
    "* [Defend the center (harder)](#defend_the_center_hard)\n",
    "\n",
    "    \n",
    "### [Part 2 - Monitoring the model](#part_2)\n",
    "* [Adding hooks](#monitoring)\n",
    "* [Monitoring callback](#callback)\n",
    "* [Adding normalization](#normalization)\n",
    "* [Comparing norms](#norm_comparison)\n",
    "    \n",
    "    \n",
    "### [Part 3 - Playing doom deathmatch](#part_3)\n",
    "* [Scenario presentation](#deathmatch_presentation)\n",
    "* [New environment](#deathmatch_environment)\n",
    "* [Discussion](#deathmatch_discussion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=part_1></div>\n",
    "\n",
    "# [^](#top) Part 1 - Increasing complexity\n",
    "\n",
    "\n",
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import cv2\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import typing as t\n",
    "import vizdoom\n",
    "from stable_baselines3 import ppo\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common import evaluation, policies\n",
    "from torch import nn\n",
    "\n",
    "from common import envs, plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "image_path1 = \"/Users/ericsunkuan/Desktop/NTUEE/112-1/RL/final_project/vizdoom/ViZDoom-master/some_tests/game_image_save/no_enemy_sample.jpeg\"\n",
    "base64_image1 = encode_image(image_path1)\n",
    "\n",
    "client = OpenAI(api_key = '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')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=defend_the_center></div>\n",
    "\n",
    "## New scenario: Defend The Center\n",
    "\n",
    "The next scenario we are going to tackle is called \"Defend the center\". In this scenario, the agent is stuck in the center of a circular arena and will be attacked by monsters spawning at random intervals and locations. Here are the rewards for this scenario:\n",
    "\n",
    "* 1 point for each ennemy killed.\n",
    "* -1 point for dying.\n",
    "\n",
    "Since there is a limit on the total ammunition of the agent, 26, the theoretical maximum reward achievable for this scenario is 25. The buttons available are `ATTACK`, `TURN_LEFT` and `TURN_RIGHT`. \n",
    "\n",
    "We are going to define a little helper function that will streamline the training and evaluation process as we will need to repeat it several time in this notebook. The function simply create the environments, instantiate an agent based on the PPO algorithm and start training and optionally evaluating the agent for a given number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_env(env_args, agent_args, n_envs, timesteps, callbacks, eval_freq=None, init_func=None):\n",
    "    \"\"\"Helper function to streamline the learning and evaluation process.\n",
    "    \n",
    "    Args:\n",
    "         env_args:    A dict containing arguments passed to the environment.\n",
    "         agent_args:  A dict containing arguments passed to the agent.\n",
    "         n_envs:      The number of parallel training envs to instantiate.\n",
    "         timesteps:   The number of timesteps for which to train the model.\n",
    "         callbacks:   A list of callbacks for the training process.\n",
    "         eval_freq:   The frequency (in steps) at which to evaluate the agent.\n",
    "         init_func:   A function to be applied on the agent before training.\n",
    "    \"\"\"\n",
    "    # Create environments.\n",
    "    env = envs.create_vec_env(n_envs, **env_args)\n",
    "\n",
    "    # Build the agent.\n",
    "    agent = ppo.PPO(policies.ActorCriticCnnPolicy, env, tensorboard_log='logs/tensorboard', seed=0, **agent_args)\n",
    "    \n",
    "    # Optional processing on the agent.\n",
    "    if init_func is not None: \n",
    "        init_func(agent)\n",
    "        \n",
    "    # Optional evaluation callback.\n",
    "    if eval_freq is not None:\n",
    "        eval_env = envs.create_eval_vec_env(**env_args)\n",
    "\n",
    "        callbacks.append(EvalCallback(\n",
    "            eval_env, \n",
    "            n_eval_episodes=10, \n",
    "            eval_freq=eval_freq, \n",
    "            log_path=f'logs/evaluations/{env_args[\"scenario\"]}',\n",
    "            best_model_save_path=f'logs/models/{env_args[\"scenario\"]}'))\n",
    "\n",
    "    # Start the training process.\n",
    "    agent.learn(total_timesteps=timesteps, tb_log_name=env_args['scenario'], callback=callbacks)\n",
    "\n",
    "    # Cleanup.\n",
    "    env.close()\n",
    "    if eval_freq is not None: eval_env.close()\n",
    "    \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we configure the environments like we have seen in the [previous notebook](https://github.com/lkiel/rl-doom/blob/develop/standalone_examples/Basic%20Scenario.ipynb). We will be training for only 100k steps as this is already enough to reach a good score in this scenario. Don't forget to specify the frame skip parameter as we have seen that this is one of the most efficient way to speed up the learning process along with the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results in a 100x156 image, no pixel lost due to padding with the default CNN architecture\n",
    "frame_processor = lambda frame: cv2.resize(frame[40:, 4:-4], None, fx=.5, fy=.5, interpolation=cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericsunkuan/anaconda3/envs/merge_test2/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ViZDoomUnexpectedExitException",
     "evalue": "Controlled ViZDoom instance exited unexpectedly.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mViZDoomUnexpectedExitException\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m env_args \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mscenario\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mbots_deathmatch_multimaps\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mframe_skip\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m4\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mframe_processor\u001b[39m\u001b[39m'\u001b[39m: frame_processor}\n\u001b[1;32m      2\u001b[0m agent_args \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mn_epochs\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m3\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mn_steps\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m2048\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1e-4\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m32\u001b[39m}\n\u001b[0;32m----> 4\u001b[0m solve_env(env_args, agent_args, n_envs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, timesteps\u001b[39m=\u001b[39;49m\u001b[39m150000\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[], eval_freq\u001b[39m=\u001b[39;49m\u001b[39m2048\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[21], line 35\u001b[0m, in \u001b[0;36msolve_env\u001b[0;34m(env_args, agent_args, n_envs, timesteps, callbacks, eval_freq, init_func)\u001b[0m\n\u001b[1;32m     27\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(EvalCallback(\n\u001b[1;32m     28\u001b[0m         eval_env, \n\u001b[1;32m     29\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, \n\u001b[1;32m     30\u001b[0m         eval_freq\u001b[39m=\u001b[39meval_freq, \n\u001b[1;32m     31\u001b[0m         log_path\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlogs/evaluations/\u001b[39m\u001b[39m{\u001b[39;00menv_args[\u001b[39m\"\u001b[39m\u001b[39mscenario\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[1;32m     32\u001b[0m         best_model_save_path\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlogs/models/\u001b[39m\u001b[39m{\u001b[39;00menv_args[\u001b[39m\"\u001b[39m\u001b[39mscenario\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m))\n\u001b[1;32m     34\u001b[0m \u001b[39m# Start the training process.\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m agent\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49mtimesteps, tb_log_name\u001b[39m=\u001b[39;49menv_args[\u001b[39m'\u001b[39;49m\u001b[39mscenario\u001b[39;49m\u001b[39m'\u001b[39;49m], callback\u001b[39m=\u001b[39;49mcallbacks)\n\u001b[1;32m     37\u001b[0m \u001b[39m# Cleanup.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m env\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/merge_test2/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[39mself\u001b[39m: SelfPPO,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 308\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    309\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    310\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    311\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    312\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    313\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    314\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    315\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/merge_test2/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:259\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 259\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[1;32m    261\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/merge_test2/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:178\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, spaces\u001b[39m.\u001b[39mBox):\n\u001b[1;32m    176\u001b[0m     clipped_actions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(actions, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mlow, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mhigh)\n\u001b[0;32m--> 178\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(clipped_actions)\n\u001b[1;32m    180\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[1;32m    182\u001b[0m \u001b[39m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/merge_test2/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:197\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \n\u001b[1;32m    193\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 197\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/merge_test2/lib/python3.10/site-packages/stable_baselines3/common/vec_env/vec_transpose.py:95\u001b[0m, in \u001b[0;36mVecTransposeImage.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m---> 95\u001b[0m     observations, rewards, dones, infos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvenv\u001b[39m.\u001b[39;49mstep_wait()\n\u001b[1;32m     97\u001b[0m     \u001b[39m# Transpose the terminal observations\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     \u001b[39mfor\u001b[39;00m idx, done \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dones):\n",
      "File \u001b[0;32m~/anaconda3/envs/merge_test2/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[39m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m     59\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[1;32m     60\u001b[0m         )\n\u001b[1;32m     61\u001b[0m         \u001b[39m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx] \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/anaconda3/envs/merge_test2/lib/python3.10/site-packages/shimmy/openai_gym_compatibility.py:251\u001b[0m, in \u001b[0;36mGymV21CompatibilityV0.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action: ActType) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[Any, \u001b[39mfloat\u001b[39m, \u001b[39mbool\u001b[39m, \u001b[39mbool\u001b[39m, \u001b[39mdict\u001b[39m]:\n\u001b[1;32m    243\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[1;32m    245\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39m        (observation, reward, terminated, truncated, info)\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m     obs, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgym_env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    253\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender()\n",
      "File \u001b[0;32m~/Desktop/NTUEE/112-1/RL/final_project/fp_merged/rl-doom/standalone_examples/common/envs.py:78\u001b[0m, in \u001b[0;36mDoomEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mTuple[Frame, \u001b[39mint\u001b[39m, \u001b[39mbool\u001b[39m, t\u001b[39m.\u001b[39mDict]:\n\u001b[1;32m     66\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Apply an action to the environment.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39m            - An empty info dict.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgame\u001b[39m.\u001b[39;49mmake_action(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpossible_actions[action], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mframe_skip)\n\u001b[1;32m     79\u001b[0m     done \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgame\u001b[39m.\u001b[39mis_episode_finished()\n\u001b[1;32m     80\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_frame(done)\n",
      "\u001b[0;31mViZDoomUnexpectedExitException\u001b[0m: Controlled ViZDoom instance exited unexpectedly."
     ]
    }
   ],
   "source": [
    "env_args = {'scenario': 'bots_deathmatch_multimaps', 'frame_skip': 4, 'frame_processor': frame_processor}\n",
    "agent_args = {'n_epochs': 3, 'n_steps': 2048, 'learning_rate': 1e-4, 'batch_size': 32}\n",
    "\n",
    "solve_env(env_args, agent_args, n_envs=2, timesteps=150000, callbacks=[], eval_freq=2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that very quickly we reach satisfactory results. This is not surprising since the scenario is not much more difficult than the basic one we have seen in the previous notebook. Let's try to add some complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=defend_the_center_hard></div>\n",
    "\n",
    "# Defend the center (difficult version)\n",
    "\n",
    "\n",
    "Remember that our ultimate goal is to train an agent to play Doom deathmatch against bots. We will use the scenario we just solved as a good middle ground to perform some experiments before jumping to the more difficult task of playing against bots. **In general, it is often a good idea to start simple and progressively add complexity to the task.**\n",
    "\n",
    "The increased complexity here will arise from the fact that in a deathmatch environment, we will need to perform more actions than simply turning around and shooting. Here is the list of buttons we will be using:\n",
    "\n",
    "```\n",
    "ATTACK, TURN_LEFT, TURN_RIGHT, MOVE_FORWARD, MOVE_LEFT, MOVE_RIGHT\n",
    "```\n",
    "\n",
    "A simple solution would be to directly add more buttons to the config file. Although this will work, you will notice that the agent will be quite slow in the sense that he can only perform one action at a time. Either shoot, turn or move. Although this is not a big issue in the current scenario, when playing later against bots running all over the place our agent will be unfairly disadvantaged because of the limitations in its actions.\n",
    "\n",
    "![Agent 6 outputs](figures/deathmatch_agent_out=6.gif)\n",
    "\n",
    "VizDoom allows us to provide a list of button states but our model uses a discrete action space. A simple workaround is to generate all possible combination of buttons and use each combination as a single action. However, the number of such possible combination is $2^n$ where $n$ is the number of available buttons. Even is our simple case with only 6 buttons, this would mean having 64 possible outputs. To mitigate the combinatorial explosion of our action space, we are going to forbid certain combination. Indeed, it does not make much sense to allow for example both `TURN_RIGHT` and `TURN_LEFT` to be activated at the same time. Therefore, we will remove the following combinations:\n",
    "\n",
    "* `TURN_RIGHT` and `TURN_LEFT`\n",
    "* `MOVE_RIGHT` and `MOVE_LEFT`\n",
    "\n",
    "And we will also only allow the `ATTACK` button to be used alone. This will not hurt performance as most weapons in Doom have a cooldown period between two attacks anyway. In the end, the total number of different possible actions is given by the cartesian product of:\n",
    "\n",
    "```\n",
    "{ATTACK} + {TURN_RIGHT, TURN_LEFT, NOTHING} x {MOVE_RIGHT, MOVE_LEFT, NOTHING} x {MOVE_FORWARD, NOTHING}\n",
    "\n",
    "```\n",
    "Since we don't want the all-zero state (no button pressed) this will give us a grand total of 18 which is much better than the 64 we could have ended up with. Here is the code to generate the action space given a list of buttons and our exclusion lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built action space of size 18 from buttons [<Button.ATTACK: 0> <Button.MOVE_FORWARD: 13> <Button.MOVE_LEFT: 11>\n",
      " <Button.MOVE_RIGHT: 10> <Button.TURN_LEFT: 15> <Button.TURN_RIGHT: 14>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 1.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 0.0, 1.0, 1.0, 0.0],\n",
       " [0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 1.0, 0.0, 0.0, 1.0],\n",
       " [0.0, 0.0, 1.0, 0.0, 1.0, 0.0],\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0, 1.0],\n",
       " [0.0, 1.0, 0.0, 0.0, 1.0, 0.0],\n",
       " [0.0, 1.0, 0.0, 1.0, 0.0, 0.0],\n",
       " [0.0, 1.0, 0.0, 1.0, 0.0, 1.0],\n",
       " [0.0, 1.0, 0.0, 1.0, 1.0, 0.0],\n",
       " [0.0, 1.0, 1.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 1.0, 1.0, 0.0, 0.0, 1.0],\n",
       " [0.0, 1.0, 1.0, 0.0, 1.0, 0.0],\n",
       " [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "import typing as t\n",
    "\n",
    "from vizdoom import Button\n",
    "\n",
    "\n",
    "# Buttons that cannot be used together\n",
    "MUTUALLY_EXCLUSIVE_GROUPS = [\n",
    "    [Button.MOVE_RIGHT, Button.MOVE_LEFT],\n",
    "    [Button.TURN_RIGHT, Button.TURN_LEFT],\n",
    "    [Button.MOVE_FORWARD, Button.MOVE_BACKWARD],\n",
    "]\n",
    "\n",
    "# Buttons that can only be used alone.\n",
    "EXCLUSIVE_BUTTONS = [Button.ATTACK]\n",
    "\n",
    "\n",
    "def has_exclusive_button(actions: np.ndarray, buttons: np.array) -> np.array:\n",
    "    exclusion_mask = np.isin(buttons, EXCLUSIVE_BUTTONS)\n",
    "    \n",
    "    # Flag actions that have more than 1 active button among exclusive list.\n",
    "    return (np.any(actions.astype(bool) & exclusion_mask, axis=-1)) & (np.sum(actions, axis=-1) > 1)\n",
    "\n",
    "\n",
    "def has_excluded_pair(actions: np.ndarray, buttons: np.array) -> np.array:\n",
    "    # Create mask of shape (n_mutual_exclusion_groups, n_available_buttons), marking location of excluded pairs.\n",
    "    mutual_exclusion_mask = np.array([np.isin(buttons, excluded_group) \n",
    "                                      for excluded_group in MUTUALLY_EXCLUSIVE_GROUPS])\n",
    "\n",
    "    # Flag actions that have more than 1 button active in any of the mutual exclusion groups.\n",
    "    return np.any(np.sum(\n",
    "        # Resulting shape (n_actions, n_mutual_exclusion_groups, n_available_buttons)\n",
    "        (actions[:, np.newaxis, :] * mutual_exclusion_mask.astype(int)),\n",
    "        axis=-1) > 1, axis=-1)\n",
    "\n",
    "\n",
    "def get_available_actions(buttons: np.array) -> t.List[t.List[float]]:\n",
    "    # Create list of all possible actions of size (2^n_available_buttons x n_available_buttons)\n",
    "    action_combinations = np.array([list(seq) for seq in itertools.product([0., 1.], repeat=len(buttons))])\n",
    "\n",
    "    # Build action mask from action combinations and exclusion mask\n",
    "    illegal_mask = (has_excluded_pair(action_combinations, buttons)\n",
    "                    | has_exclusive_button(action_combinations, buttons))\n",
    "\n",
    "    possible_actions = action_combinations[~illegal_mask]\n",
    "    possible_actions = possible_actions[np.sum(possible_actions, axis=1) > 0]  # Remove no-op\n",
    "\n",
    "    print('Built action space of size {} from buttons {}'.format(len(possible_actions), buttons))\n",
    "    return possible_actions.tolist()\n",
    "\n",
    "possible_actions = get_available_actions(np.array([\n",
    "    Button.ATTACK, Button.MOVE_FORWARD, Button.MOVE_LEFT, \n",
    "    Button.MOVE_RIGHT, Button.TURN_LEFT, Button.TURN_RIGHT]))\n",
    "\n",
    "possible_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what a trained agent the uses the modified 18-output version navigate across the map. It looks much more dynamic and we can expect better results when competing in a deathmatch.\n",
    "\n",
    "![Agent 18 outputs](figures/deathmatch_agent_out=18.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model in this more challenging environment will definitely require more training time. This is a good opportunity to log some metrics to help us monitor the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=part_2></div>\n",
    "\n",
    "# [^](#top) Part 2 - Monitoring the model\n",
    "\n",
    "Stable-baselines already logs several useful metrics for us such as the value, policy or entropy losses, mean episode length and mean episode reward. The next most useful quantity to keep an eye on is the evolution of the model weights and activations. In particular, we'd like to make sure that our activations are well behaved. That is, they are not always zero (which would suggest we have dead neurons) and that the variance of each layer output stays away from zero as well. If this is not the case, we might end up with vanishing gradient issues and ultimately poor learning performance.\n",
    "\n",
    "I already mentioned the amazing course from FastAI's in the previous part of this series. We are going to implement several suggestions from the [10th lesson of the course](https://course19.fast.ai/videos/?lesson=10). Namely:\n",
    "\n",
    "* Activation layer monitoring with hooks\n",
    "* Kaiming initialization\n",
    "* Batch/Layer normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=monitoring></div>\n",
    "\n",
    "## Hooks\n",
    "\n",
    "Let's start by tracking neuron activations. This is particularly important for the first layers of our model. To do this, we will use FastAI's approach of defining hook wrappers. A hook is a function that can be attached to a specific layer of our model and that will be called for each forward pass. This is a feature offered by PyTorch and the idea is to build on top of that to keep track of useful statistics.\n",
    "\n",
    "We will combine this approach with Tensorboard ability to log tensors to get the distribution over time of our layer activations. Our hooks will contain a single attribute, `activation_data`, that will store the layer output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from functools import partial\n",
    "\n",
    "class Hook:\n",
    "    \"\"\"Wrapper for PyTorch forward hook mechanism.\"\"\"\n",
    "    def __init__(self, module: nn.Module, func: t.Callable):\n",
    "        self.hook = None            # PyTorch's hook.\n",
    "        self.module = module        # PyTorch layer to which the hook is attached to.\n",
    "        self.func = func            # Function to call on each forward pass.\n",
    "        self.register()\n",
    "\n",
    "    def register(self):\n",
    "        self.activation_data = deque(maxlen=1024)\n",
    "        self.hook = self.module.register_forward_hook(partial(self.func, self))\n",
    "\n",
    "    def remove(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "def store_activation(hook, module, inp, outp):\n",
    "    \"\"\"Function intented to be called by a hook on a forward pass.\n",
    "    \n",
    "    Args:\n",
    "        hook:    The hook object that generated the call.\n",
    "        module:  The module on which the hook is registered.\n",
    "        inp:     Input of the module.\n",
    "        outp:    Output of the module.\n",
    "    \"\"\"\n",
    "    hook.activation_data.append(outp.data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=callback></div>\n",
    "\n",
    "## Monitoring callback\n",
    "\n",
    "The second step is to define a callback that will periodically log hook data. For simplicity, we will only be logging in Tensorboard the latest activation value of each training phase. \n",
    "\n",
    "Of course, if you need more granularity, you can easily log more values. Keep in mind though that stable-baselines logger implementation aligns everything on the rollout time steps (which do not correspond to forward passes in training) when logging to Tensorboard. So you will need to do some adjustments there to either log to a file yourself or to adapt stable-baseline's implementation. You can also take a look at the [plotting helpers](common/plotting.py) which make use of the complete list of activation to create the diagnostic plots below.\n",
    "\n",
    "The callback will also compute and log some basic statistics:\n",
    "\n",
    "* The mean of each activation layer\n",
    "* The standard deviation of each activation layer\n",
    "* The proportion of activations that are between -0.2 and 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "\n",
    "def get_low_act(data, threshold=0.2):\n",
    "    \"\"\"Computes the proportion of activations that have value close to zero.\"\"\"\n",
    "    low_activation = ((-threshold <= data) & (data <= threshold))\n",
    "    return np.count_nonzero(low_activation) / np.size(low_activation)\n",
    "\n",
    "\n",
    "# Callback for periodic logging to tensorboard.\n",
    "class LayerActivationMonitoring(BaseCallback):\n",
    "    \n",
    "    def _on_rollout_start(self) -> None:\n",
    "        \"\"\"Called after the training phase.\"\"\"\n",
    "        \n",
    "        hooks = self.model.policy.features_extractor.hooks\n",
    "        \n",
    "        # Remove the hooks so that they don't get called for rollout collection.\n",
    "        for h in hooks: h.remove() \n",
    "\n",
    "        # Log last datapoint and statistics to tensorboard.\n",
    "        for i, hook in enumerate(hooks):\n",
    "            if len(hook.activation_data) > 0:\n",
    "                data = hook.activation_data[-1]\n",
    "                self.logger.record(f'diagnostics/activation_l{i}', data)\n",
    "                self.logger.record(f'diagnostics/mean_l{i}', np.mean(data))\n",
    "                self.logger.record(f'diagnostics/std_l{i}', np.std(data))\n",
    "                self.logger.record(f'diagnostics/low_act_prop_l{i}', get_low_act(data))\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \"\"\"Called before the training phase.\"\"\"\n",
    "        for h in self.model.policy.features_extractor.hooks: h.register()\n",
    "\n",
    "    def _on_step(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define a function that will attach hooks to every activation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_hooks(model):\n",
    "    model.policy.features_extractor.hooks = [\n",
    "        Hook(layer, store_activation)\n",
    "        for layer in model.policy.features_extractor.cnn\n",
    "        if isinstance(layer, nn.ReLU) or isinstance(layer, nn.LeakyReLU)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe the behaviour of the model using our new logging mechanism. We train the model for a few steps and let the hook collect the activations during the forward passes. Note that for the PPO algorithm, we first need to collect a rollout sample using the current policy before any training can take place. Logging those activation is not very useful as the layer weights are not being updated yet. This is why we detach and reattach the hooks in the `_on_rollout_start` and `_on_rollout_end` functions respectively from the callback above. \n",
    "\n",
    "The helper function below will simply extract the list of layer outputs collected by the hook, compute and then plot some statistics. Using the idea presented in FastAI's course, it will also plot an histogram of the layer activations. The horizontal axis represents training steps (processing of one minibatch), in chronological order from left to right. The vertical axis represents the histogram bins from -7 to 7 (bottom to top). Brighter regions indicate larger histogram frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ViZDoomUnexpectedExitException",
     "evalue": "Controlled ViZDoom instance exited unexpectedly.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mViZDoomUnexpectedExitException\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Train for a few steps to collect the activations.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m agent \u001b[39m=\u001b[39m solve_env(env_args, agent_args, \u001b[39m1\u001b[39;49m, \u001b[39m1024\u001b[39;49m, [LayerActivationMonitoring()], init_func\u001b[39m=\u001b[39;49mregister_hooks)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Plot statistics.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m plotting\u001b[39m.\u001b[39mplot_activations(agent\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39mfeatures_extractor\u001b[39m.\u001b[39mhooks)\n",
      "Cell \u001b[0;32mIn[21], line 35\u001b[0m, in \u001b[0;36msolve_env\u001b[0;34m(env_args, agent_args, n_envs, timesteps, callbacks, eval_freq, init_func)\u001b[0m\n\u001b[1;32m     27\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(EvalCallback(\n\u001b[1;32m     28\u001b[0m         eval_env, \n\u001b[1;32m     29\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, \n\u001b[1;32m     30\u001b[0m         eval_freq\u001b[39m=\u001b[39meval_freq, \n\u001b[1;32m     31\u001b[0m         log_path\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlogs/evaluations/\u001b[39m\u001b[39m{\u001b[39;00menv_args[\u001b[39m\"\u001b[39m\u001b[39mscenario\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[1;32m     32\u001b[0m         best_model_save_path\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlogs/models/\u001b[39m\u001b[39m{\u001b[39;00menv_args[\u001b[39m\"\u001b[39m\u001b[39mscenario\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m))\n\u001b[1;32m     34\u001b[0m \u001b[39m# Start the training process.\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m agent\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49mtimesteps, tb_log_name\u001b[39m=\u001b[39;49menv_args[\u001b[39m'\u001b[39;49m\u001b[39mscenario\u001b[39;49m\u001b[39m'\u001b[39;49m], callback\u001b[39m=\u001b[39;49mcallbacks)\n\u001b[1;32m     37\u001b[0m \u001b[39m# Cleanup.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m env\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/merge_test2/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[39mself\u001b[39m: SelfPPO,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 308\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    309\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    310\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    311\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    312\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    313\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    314\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    315\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/merge_test2/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:259\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 259\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[1;32m    261\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/merge_test2/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:178\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, spaces\u001b[39m.\u001b[39mBox):\n\u001b[1;32m    176\u001b[0m     clipped_actions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(actions, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mlow, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mhigh)\n\u001b[0;32m--> 178\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(clipped_actions)\n\u001b[1;32m    180\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[1;32m    182\u001b[0m \u001b[39m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/merge_test2/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:197\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \n\u001b[1;32m    193\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 197\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/merge_test2/lib/python3.10/site-packages/stable_baselines3/common/vec_env/vec_transpose.py:95\u001b[0m, in \u001b[0;36mVecTransposeImage.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m---> 95\u001b[0m     observations, rewards, dones, infos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvenv\u001b[39m.\u001b[39;49mstep_wait()\n\u001b[1;32m     97\u001b[0m     \u001b[39m# Transpose the terminal observations\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     \u001b[39mfor\u001b[39;00m idx, done \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dones):\n",
      "File \u001b[0;32m~/anaconda3/envs/merge_test2/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[39m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m     59\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[1;32m     60\u001b[0m         )\n\u001b[1;32m     61\u001b[0m         \u001b[39m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx] \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/anaconda3/envs/merge_test2/lib/python3.10/site-packages/shimmy/openai_gym_compatibility.py:251\u001b[0m, in \u001b[0;36mGymV21CompatibilityV0.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action: ActType) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[Any, \u001b[39mfloat\u001b[39m, \u001b[39mbool\u001b[39m, \u001b[39mbool\u001b[39m, \u001b[39mdict\u001b[39m]:\n\u001b[1;32m    243\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[1;32m    245\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39m        (observation, reward, terminated, truncated, info)\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m     obs, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgym_env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    253\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender()\n",
      "File \u001b[0;32m~/Desktop/NTUEE/112-1/RL/final_project/fp_merged/rl-doom/standalone_examples/common/envs.py:78\u001b[0m, in \u001b[0;36mDoomEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mTuple[Frame, \u001b[39mint\u001b[39m, \u001b[39mbool\u001b[39m, t\u001b[39m.\u001b[39mDict]:\n\u001b[1;32m     66\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Apply an action to the environment.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39m            - An empty info dict.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgame\u001b[39m.\u001b[39;49mmake_action(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpossible_actions[action], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mframe_skip)\n\u001b[1;32m     79\u001b[0m     done \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgame\u001b[39m.\u001b[39mis_episode_finished()\n\u001b[1;32m     80\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_frame(done)\n",
      "\u001b[0;31mViZDoomUnexpectedExitException\u001b[0m: Controlled ViZDoom instance exited unexpectedly."
     ]
    }
   ],
   "source": [
    "# Train for a few steps to collect the activations.\n",
    "agent = solve_env(env_args, agent_args, 1, 1024, [LayerActivationMonitoring()], init_func=register_hooks)\n",
    "\n",
    "# Plot statistics.\n",
    "plotting.plot_activations(agent.policy.features_extractor.hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice at the beginning of the training phase several potential issues. First, the variance of the output of each layer is very close to zero and there is a noticeable decrease between the first and the last layer. Also, almost all the outputs are near zero which can be seen both on the graph showing the proportion of small activations over time and the activation histogram.\n",
    "\n",
    "In addition to using LeakyReLU instead of ReLU and following FastAI's recommendation, we will perform two changes:\n",
    "\n",
    "1. Use Kaiming initialization\n",
    "2. Use some normalization (layer norm in our case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_net(m: nn.Module):\n",
    "    if len(m._modules) > 0:\n",
    "        for subm in m._modules:\n",
    "            init_net(m._modules[subm])\n",
    "    elif isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(\n",
    "            m.weight, \n",
    "            a=0.1,         # Same as the leakiness parameter for LeakyReLu.\n",
    "            mode='fan_in', # Preserves magnitude in the forward pass.\n",
    "            nonlinearity='leaky_relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=normalization></div>\n",
    "\n",
    "## Normalization\n",
    "\n",
    "To add layer normalization, we need to redefine a model. We will then tell stable-baselines to use our customized model instead of the default one. For the sake of brevity, sizes for the different layers of our model have been hardcoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "class CustomCNN(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 128, **kwargs):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.LayerNorm([3, 100, 156]),\n",
    "            \n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4, padding=0, bias=False),\n",
    "            nn.LayerNorm([32, 24, 38]),\n",
    "            nn.LeakyReLU(**kwargs),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0, bias=False),\n",
    "            nn.LayerNorm([64, 11, 18]),\n",
    "            nn.LeakyReLU(**kwargs),\n",
    "            \n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0, bias=False),\n",
    "            nn.LayerNorm([64, 9, 16]),\n",
    "            nn.LeakyReLU(**kwargs),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(9216, features_dim, bias=False),\n",
    "            nn.LayerNorm(features_dim),\n",
    "            nn.LeakyReLU(**kwargs),\n",
    "        )\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        return self.linear(self.cnn(observations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's collect some data again but this time using the modified model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent_args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     register_hooks(model)\n\u001b[1;32m      3\u001b[0m     init_net(model\u001b[39m.\u001b[39mpolicy)\n\u001b[0;32m----> 5\u001b[0m agent_args[\u001b[39m'\u001b[39m\u001b[39mpolicy_kwargs\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mfeatures_extractor_class\u001b[39m\u001b[39m'\u001b[39m: CustomCNN}    \n\u001b[1;32m      6\u001b[0m agent \u001b[39m=\u001b[39m solve_env(env_args, agent_args, \u001b[39m1\u001b[39m, \u001b[39m1024\u001b[39m, [LayerActivationMonitoring()], init_func\u001b[39m=\u001b[39minit_model)\n\u001b[1;32m      8\u001b[0m plotting\u001b[39m.\u001b[39mplot_activations(agent\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39mfeatures_extractor\u001b[39m.\u001b[39mhooks)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'agent_args' is not defined"
     ]
    }
   ],
   "source": [
    "def init_model(model):\n",
    "    register_hooks(model)\n",
    "    init_net(model.policy)\n",
    "    \n",
    "agent_args['policy_kwargs'] = {'features_extractor_class': CustomCNN}    \n",
    "agent = solve_env(env_args, agent_args, 1, 1024, [LayerActivationMonitoring()], init_func=init_model)\n",
    "\n",
    "plotting.plot_activations(agent.policy.features_extractor.hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice now that the variance is better behaved, hovering around 0.6. The proportion of activations that are close to zero is significantly lower at ~65% instead of nearly 90% before. In particular, we can notice that the model is now using the full range of the ReLU activation function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=norm_comparison></div>\n",
    "\n",
    "## Comparing the models\n",
    "\n",
    "The metrics associated with our model look much better now. But does this translate into better performance? In FastAI example, adding normalization was the most impactful factor in improving the results. To see whether we get similar benefits in our setup, let's solve the more difficult variable variant of \"defend the center\" with different parameters: no normalization, layer normalization and batch normalization. In the plot below, each line is the average of 6 consecutive trials. The coloured region denotes the error mean: $ \\frac{\\sigma}{\\sqrt{n}}$.\n",
    "\n",
    "Unfortunately, although our modified model exhibits nicer telemetry, it seems to have very little impact on the training performance. We will keep the changes nonetheless since we know they help with the fundamentals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Comparison normalization](figures/comparison_norm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, here is the final setup to train against the more difficult verion of \"defend the center\". It includes the modified architecture with layer normalization as well as the layer activation monitoring callback that will periodically save to tensorboard. To reach satisfactory results in this more difficult environment, you will need to let the agent train for roughly 500k steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=8192, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 107.00 +/- 24.96\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16384, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 94.00 +/- 16.63\n",
      "Eval num_timesteps=24576, episode_reward=-0.70 +/- 0.46\n",
      "Episode length: 88.70 +/- 7.28\n",
      "New best mean reward!\n",
      "Eval num_timesteps=32768, episode_reward=-0.70 +/- 0.64\n",
      "Episode length: 91.70 +/- 13.19\n",
      "Eval num_timesteps=40960, episode_reward=0.50 +/- 1.20\n",
      "Episode length: 105.30 +/- 36.72\n",
      "New best mean reward!\n",
      "Eval num_timesteps=49152, episode_reward=1.60 +/- 0.80\n",
      "Episode length: 112.90 +/- 11.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=57344, episode_reward=8.30 +/- 4.78\n",
      "Episode length: 255.00 +/- 102.23\n",
      "New best mean reward!\n",
      "Eval num_timesteps=65536, episode_reward=3.70 +/- 2.41\n",
      "Episode length: 154.50 +/- 59.59\n",
      "Eval num_timesteps=73728, episode_reward=4.70 +/- 2.53\n",
      "Episode length: 164.20 +/- 45.98\n",
      "Eval num_timesteps=81920, episode_reward=7.40 +/- 5.24\n",
      "Episode length: 212.10 +/- 109.29\n",
      "Eval num_timesteps=90112, episode_reward=21.10 +/- 3.01\n",
      "Episode length: 463.70 +/- 65.06\n",
      "New best mean reward!\n",
      "Eval num_timesteps=98304, episode_reward=16.10 +/- 8.09\n",
      "Episode length: 377.40 +/- 159.12\n",
      "Eval num_timesteps=106496, episode_reward=22.90 +/- 2.59\n",
      "Episode length: 510.60 +/- 32.05\n",
      "New best mean reward!\n",
      "Eval num_timesteps=114688, episode_reward=22.80 +/- 1.08\n",
      "Episode length: 502.20 +/- 38.03\n",
      "Eval num_timesteps=122880, episode_reward=22.40 +/- 1.91\n",
      "Episode length: 489.50 +/- 29.52\n",
      "Eval num_timesteps=131072, episode_reward=23.10 +/- 1.45\n",
      "Episode length: 506.60 +/- 43.25\n",
      "New best mean reward!\n",
      "Eval num_timesteps=139264, episode_reward=20.80 +/- 2.68\n",
      "Episode length: 449.30 +/- 61.72\n",
      "Eval num_timesteps=147456, episode_reward=23.10 +/- 1.45\n",
      "Episode length: 492.70 +/- 32.17\n",
      "Eval num_timesteps=155648, episode_reward=22.70 +/- 1.10\n",
      "Episode length: 492.30 +/- 29.32\n",
      "Eval num_timesteps=163840, episode_reward=24.00 +/- 0.89\n",
      "Episode length: 489.10 +/- 44.19\n",
      "New best mean reward!\n",
      "Eval num_timesteps=172032, episode_reward=23.00 +/- 0.63\n",
      "Episode length: 457.00 +/- 56.27\n",
      "Eval num_timesteps=180224, episode_reward=23.10 +/- 1.45\n",
      "Episode length: 486.70 +/- 47.28\n",
      "Eval num_timesteps=188416, episode_reward=21.90 +/- 1.30\n",
      "Episode length: 478.80 +/- 52.28\n",
      "Eval num_timesteps=196608, episode_reward=23.20 +/- 1.25\n",
      "Episode length: 479.10 +/- 53.99\n",
      "Eval num_timesteps=204800, episode_reward=24.20 +/- 0.75\n",
      "Episode length: 499.60 +/- 45.08\n",
      "New best mean reward!\n",
      "Eval num_timesteps=212992, episode_reward=23.80 +/- 1.47\n",
      "Episode length: 499.50 +/- 44.55\n",
      "Eval num_timesteps=221184, episode_reward=22.70 +/- 1.85\n",
      "Episode length: 458.10 +/- 46.97\n",
      "Eval num_timesteps=229376, episode_reward=22.70 +/- 1.49\n",
      "Episode length: 422.00 +/- 61.13\n",
      "Eval num_timesteps=237568, episode_reward=23.40 +/- 1.50\n",
      "Episode length: 444.30 +/- 66.48\n",
      "Eval num_timesteps=245760, episode_reward=21.70 +/- 2.41\n",
      "Episode length: 438.60 +/- 65.30\n",
      "Eval num_timesteps=253952, episode_reward=23.00 +/- 1.48\n",
      "Episode length: 434.40 +/- 63.06\n",
      "Eval num_timesteps=262144, episode_reward=23.40 +/- 2.29\n",
      "Episode length: 442.10 +/- 85.72\n",
      "Eval num_timesteps=270336, episode_reward=23.10 +/- 1.70\n",
      "Episode length: 487.20 +/- 61.11\n",
      "Eval num_timesteps=278528, episode_reward=22.90 +/- 1.81\n",
      "Episode length: 442.80 +/- 71.75\n",
      "Eval num_timesteps=286720, episode_reward=23.20 +/- 2.23\n",
      "Episode length: 478.10 +/- 71.42\n",
      "Eval num_timesteps=294912, episode_reward=23.20 +/- 1.99\n",
      "Episode length: 470.80 +/- 83.41\n",
      "Eval num_timesteps=303104, episode_reward=22.60 +/- 1.11\n",
      "Episode length: 463.10 +/- 80.52\n"
     ]
    }
   ],
   "source": [
    "env_args = {\n",
    "    'scenario': 'defend_the_center_relaxed', \n",
    "    'frame_skip': 4, \n",
    "    'frame_processor': frame_processor\n",
    "}\n",
    "\n",
    "agent_args = {\n",
    "    'n_epochs': 3,\n",
    "    'n_steps': 4096,\n",
    "    'learning_rate': 1e-4,\n",
    "    'batch_size': 32,\n",
    "    'policy_kwargs': {'features_extractor_class': CustomCNN}\n",
    "}\n",
    "\n",
    "agent = solve_env(env_args, \n",
    "                  agent_args, \n",
    "                  n_envs=2, \n",
    "                  timesteps=300000, \n",
    "                  callbacks=[LayerActivationMonitoring()], \n",
    "                  eval_freq=4096, \n",
    "                  init_func=init_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=part_3></div>\n",
    "\n",
    "# [^](#top) Part 3 - Playing against bots \n",
    "\n",
    "<div id=deathmatch_presentation></div>\n",
    "\n",
    "## Scenario presentation\n",
    "\n",
    "We are now ready to tackle the task of playing Doom deathmatch against programmed bots! To play in deathmatch mode, we need a proper map with items like ammunition, health and respawn points. Using [Slade](https://slade.mancubus.net/index.php?page=about), I created a simple map to train the agent on. It is designed to be easy to navigate through while still requiring some movement to find ennemies. There are 10 spawn points, several ammunition and health items. Players can increase their firepower by picking up one of the four shotguns scattered across the map. The map is contained in the `bots_deathmatch_v1.wad`, next to the corresponding `bots_deathmatch_v1.cfg` scenario config.\n",
    "\n",
    "A game of deatchmatch consists in a session of 2:30 minutes during which 8 bots in addition of our agent will fight against each other in order to get the most frags. A \"frag\" corresponds to an enemy killed. Each player starts with a pistol and a couple of second of invicinbility. As soon as the agent dies, it is respawned at one of the respawn point picked uniformly at random. The actions available are the same as the difficul variant of \"defend the center\". That is, the agent can move, turn and shoot according to the rules we defined earlier. \n",
    "\n",
    "The rewards here will be directly obtained from the frags obtained by the agent: 1 frag = 1 point. Note that in the case of a suicide (which can happen when using rocket launchers for example), the frag count will decrease by one. Conveniently, this will work as a penalty to discourage the agent of ending with such outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Map presentation](figures/flatmap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=deathmatch_environment></div>\n",
    "\n",
    "## Modified environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up a deathmatch game requires a slightly different config for VizDoom as we need to tell it to launch a host for the multiplayer game with a couple of multiplayer-specific parameters.\n",
    "\n",
    "We also need to adapt the environment wrapper to handle bots and the deathmatch mechanics. In particular, we need to keep track of the \"frags\" obtained by our agent as this constitutes our reward basis. \n",
    "\n",
    "The cell below shows how we can adapt the environment wrapper developped so far to make it compatible for a deathmatch game. The code is pretty self-explanatory. Notice how we redefine the action space in the constructor using the button combinations. Also, to add and remove bots we use the `send_game_command` from the VizDoom game instance.\n",
    "\n",
    "**Important note: for the following to work, you will need to have the `bots.cfg` file in the same directory as the one used for execution (the notebook in this case). Otherwise, adding bots will have no effect!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import spaces\n",
    "from vizdoom.vizdoom import GameVariable\n",
    "\n",
    "class DoomWithBots(envs.DoomEnv):\n",
    "\n",
    "    \"\"\"\n",
    "    define the logic of when to prompt LLM\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, game, frame_processor, frame_skip, n_bots):\n",
    "        super().__init__(game, frame_processor, frame_skip)\n",
    "        self.n_bots = n_bots\n",
    "        self.last_frags = 0    \n",
    "        self._reset_bots()\n",
    "        \n",
    "        # Redefine the action space using combinations.\n",
    "        self.possible_actions = get_available_actions(np.array(game.get_available_buttons()))\n",
    "        self.action_space = spaces.Discrete(len(self.possible_actions))\n",
    "\n",
    "        self.sampled_actions_done = False\n",
    "        self.cur_actions = 0  ### the current action network num\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        ### prompt LLM if actions are done :\n",
    "        #if self.sampled_actions_done:\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.game.make_action(self.possible_actions[action], self.frame_skip)\n",
    "       \n",
    "        # Compute rewards.\n",
    "        frags = self.game.get_game_variable(GameVariable.FRAGCOUNT)\n",
    "        reward = frags - self.last_frags\n",
    "        self.last_frags = frags\n",
    "\n",
    "        # Check for episode end.\n",
    "        self._respawn_if_dead()\n",
    "        done = self.game.is_episode_finished()\n",
    "        self.state = self._get_frame(done)\n",
    "\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self._reset_bots()\n",
    "        self.last_frags = 0\n",
    "\n",
    "        return super().reset()\n",
    "\n",
    "    def _respawn_if_dead(self):\n",
    "        if not self.game.is_episode_finished():\n",
    "            if self.game.is_player_dead():\n",
    "                self.game.respawn_player()\n",
    "                \n",
    "    def _reset_bots(self):\n",
    "        # Make sure you have the bots.cfg file next to the program entry point.\n",
    "        self.game.send_game_command('removebots')\n",
    "        for i in range(self.n_bots):\n",
    "            self.game.send_game_command('addbot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other adaptation required is to pass several game arguments via the `add_game_args` function when creating the actual VizDoom game instance. We also redefine the function creating vectorized environments so that it uses the newly defined wrapper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import VecTransposeImage, DummyVecEnv\n",
    "\n",
    "def env_with_bots(scenario, **kwargs) -> envs.DoomEnv:\n",
    "    # Create a VizDoom instance.\n",
    "    game = vizdoom.DoomGame()\n",
    "    game.load_config(f'scenarios/{scenario}.cfg')\n",
    "    game.add_game_args('-host 1 -deathmatch +viz_nocheat 0 +cl_run 1 +name AGENT +colorset 0' +\n",
    "                       '+sv_forcerespawn 1 +sv_respawnprotect 1 +sv_nocrouch 1 +sv_noexit 1')      # Players can't crouch.\n",
    "    game.init()\n",
    "\n",
    "    return DoomWithBots(game, **kwargs)\n",
    "\n",
    "def vec_env_with_bots(n_envs=1, **kwargs) -> VecTransposeImage:\n",
    "    return VecTransposeImage(DummyVecEnv([lambda: env_with_bots(**kwargs)] * n_envs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it, we have all the elements required to play Doom deathmatch! Let us define one final time the environment and agent arguments and launch the training process using our latest model.\n",
    "\n",
    "Training an agent to play deathmatch is significantly harder than the previous scenarios we have tried so far. We will need to be patient and train for a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built action space of size 18 from buttons [<Button.ATTACK: 0> <Button.MOVE_FORWARD: 13> <Button.MOVE_LEFT: 11>\n",
      " <Button.MOVE_RIGHT: 10> <Button.TURN_RIGHT: 14> <Button.TURN_LEFT: 15>]\n",
      "Built action space of size 18 from buttons [<Button.ATTACK: 0> <Button.MOVE_FORWARD: 13> <Button.MOVE_LEFT: 11>\n",
      " <Button.MOVE_RIGHT: 10> <Button.TURN_RIGHT: 14> <Button.TURN_LEFT: 15>]\n"
     ]
    },
    {
     "ename": "ViZDoomUnexpectedExitException",
     "evalue": "Controlled ViZDoom instance exited unexpectedly.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mViZDoomUnexpectedExitException\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 37\u001b[0m\n\u001b[1;32m     29\u001b[0m eval_callback \u001b[39m=\u001b[39m EvalCallback(\n\u001b[1;32m     30\u001b[0m     eval_env, \n\u001b[1;32m     31\u001b[0m     n_eval_episodes\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, \n\u001b[1;32m     32\u001b[0m     eval_freq\u001b[39m=\u001b[39m\u001b[39m16384\u001b[39m, \n\u001b[1;32m     33\u001b[0m     log_path\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlogs/evaluations/\u001b[39m\u001b[39m{\u001b[39;00menv_args[\u001b[39m\"\u001b[39m\u001b[39mscenario\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[1;32m     34\u001b[0m     best_model_save_path\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlogs/models/\u001b[39m\u001b[39m{\u001b[39;00menv_args[\u001b[39m\"\u001b[39m\u001b[39mscenario\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[39m# Start the training process.\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m agent\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m3000000\u001b[39;49m, tb_log_name\u001b[39m=\u001b[39;49menv_args[\u001b[39m'\u001b[39;49m\u001b[39mscenario\u001b[39;49m\u001b[39m'\u001b[39;49m], callback\u001b[39m=\u001b[39;49m[monitoring_callback, eval_callback])\n\u001b[1;32m     39\u001b[0m \u001b[39m# Cleanup.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m env\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/merge_test2/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[39mself\u001b[39m: SelfPPO,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 308\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    309\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    310\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    311\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    312\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    313\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    314\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    315\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/merge_test2/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:259\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 259\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[1;32m    261\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/merge_test2/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:178\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, spaces\u001b[39m.\u001b[39mBox):\n\u001b[1;32m    176\u001b[0m     clipped_actions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(actions, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mlow, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mhigh)\n\u001b[0;32m--> 178\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(clipped_actions)\n\u001b[1;32m    180\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[1;32m    182\u001b[0m \u001b[39m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/merge_test2/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:197\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \n\u001b[1;32m    193\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 197\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/merge_test2/lib/python3.10/site-packages/stable_baselines3/common/vec_env/vec_transpose.py:95\u001b[0m, in \u001b[0;36mVecTransposeImage.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m---> 95\u001b[0m     observations, rewards, dones, infos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvenv\u001b[39m.\u001b[39;49mstep_wait()\n\u001b[1;32m     97\u001b[0m     \u001b[39m# Transpose the terminal observations\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     \u001b[39mfor\u001b[39;00m idx, done \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dones):\n",
      "File \u001b[0;32m~/anaconda3/envs/merge_test2/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[39m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m     59\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[1;32m     60\u001b[0m         )\n\u001b[1;32m     61\u001b[0m         \u001b[39m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx] \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/anaconda3/envs/merge_test2/lib/python3.10/site-packages/shimmy/openai_gym_compatibility.py:251\u001b[0m, in \u001b[0;36mGymV21CompatibilityV0.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action: ActType) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[Any, \u001b[39mfloat\u001b[39m, \u001b[39mbool\u001b[39m, \u001b[39mbool\u001b[39m, \u001b[39mdict\u001b[39m]:\n\u001b[1;32m    243\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[1;32m    245\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39m        (observation, reward, terminated, truncated, info)\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m     obs, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgym_env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    253\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender()\n",
      "Cell \u001b[0;32mIn[11], line 38\u001b[0m, in \u001b[0;36mDoomWithBots.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     30\u001b[0m     \n\u001b[1;32m     31\u001b[0m     \u001b[39m### prompt LLM if actions are done :\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[39m#if self.sampled_actions_done:\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgame\u001b[39m.\u001b[39;49mmake_action(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpossible_actions[action], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mframe_skip)\n\u001b[1;32m     40\u001b[0m     \u001b[39m# Compute rewards.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     frags \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgame\u001b[39m.\u001b[39mget_game_variable(GameVariable\u001b[39m.\u001b[39mFRAGCOUNT)\n",
      "\u001b[0;31mViZDoomUnexpectedExitException\u001b[0m: Controlled ViZDoom instance exited unexpectedly."
     ]
    }
   ],
   "source": [
    "# Define new environment parameters.\n",
    "env_args = {\n",
    "    'scenario': 'bots_deathmatch_multimaps',\n",
    "    'frame_skip': 1,\n",
    "    'frame_processor': frame_processor,\n",
    "    'n_bots': 8\n",
    "}\n",
    "\n",
    "# Defines new agent parameters.\n",
    "agent_args = {\n",
    "    'n_epochs': 3,\n",
    "    'n_steps': 4096,\n",
    "    'learning_rate': 1e-4,\n",
    "    'batch_size': 32,\n",
    "    'policy_kwargs': {'features_extractor_class': CustomCNN}\n",
    "}\n",
    "\n",
    "# Create environments with bots.\n",
    "env = vec_env_with_bots(1, **env_args)\n",
    "eval_env = vec_env_with_bots(1, **env_args)\n",
    "\n",
    "# Build the agent.\n",
    "agent = ppo.PPO(policies.ActorCriticCnnPolicy, env, tensorboard_log='logs/tensorboard', seed=0, **agent_args)\n",
    "init_model(agent)\n",
    "\n",
    "# Create callbacks.\n",
    "monitoring_callback = LayerActivationMonitoring()\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env, \n",
    "    n_eval_episodes=10, \n",
    "    eval_freq=16384, \n",
    "    log_path=f'logs/evaluations/{env_args[\"scenario\"]}',\n",
    "    best_model_save_path=f'logs/models/{env_args[\"scenario\"]}')\n",
    "\n",
    "# Start the training process.\n",
    "agent.learn(total_timesteps=3000000, tb_log_name=env_args['scenario'], callback=[monitoring_callback, eval_callback])\n",
    "\n",
    "# Cleanup.\n",
    "env.close()\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how did the agent perform? Plotting the average reward over time depicts a tragic situation: although there seems to be some improvement over time, the performance of our model is really not great. Even after 2 million steps the agent barely reaches 2 frags per match on average. Compare that to the best bot which has around 13 frags at the end of each game and our objective seem still pretty far away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotting.plot_evaluation_results('logs/evaluations/deathmatch_simple/evaluations.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=deathmatch_discussion></div>\n",
    "\n",
    "# Discussion\n",
    "\n",
    "So the learning process did not go as smoothly as one could have expected. Why is that?\n",
    "\n",
    "The issue lies in the fact that the rewards are very sparse. In other words, only a few combinations of state and action generate useful signals that our agent can use for learning. Indeed, to obtain a frag, the agent has to perform several steps \"just right\". It has to move and track ennemies, repeatedly shoot them until eventually their health reaches zero.\n",
    "\n",
    "The way to solve this issue is by means of \"reward shaping\". The idea is simple: give small positive reward to action that we believe will help towards our main objective of obtaining frags. For example, we can give rewards proportional to damage inflicted to ennemies or proportional to the ammo and health collected.\n",
    "\n",
    "Another option to help with the learning process is to design some learning curriculum. The idea here is to simplify the task early in the learning process and gradually increase the difficulty. For example, we could reduce the speed and the health of ennemies to make it easier for our agent to obtain positive rewards.\n",
    "\n",
    "We will implement these ideas in the next part of this series to get much better results so stay tuned!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
